name: test-run

trainer:
  model_url: https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/animesfw.tgz
  output_path: checkpoint
  init_batch_size: 1
  resolution: 512
  center_crop: false
  precision: fp32
  seed: 1138
  gradient_checkpointing: true
  clip_skip: 2
  pad_tokens: true
  use_ema: false
  use_hivemind: true

checkpoint:
  monitor: 'train_loss'
  dirpath: checkpoint
  filename: 'sample-nd-epoch{epoch:02d}-val_loss{val/loss:.2f}'
  auto_insert_metric_name: false
  every_n_epochs: 0
  save_top_k: 2
  save_last: true

hivemind:
  target_batch_size: 1000
  verbose: true
  # initial_peers: ['/ip4/159.223.171.199/tcp/41555/p2p/QmVQks3jCMEyakLtayUxtFXohpunqiMdWuAWpXYo26vrdV']
  run_id: "test-run-1112b"
  host_maddrs: ["/ip4/0.0.0.0/tcp/0", "/ip4/0.0.0.0/udp/0/quic"]
  reuse_grad_buffers: true
  offload_optimizer: true


lightning:
  accelerator: gpu
  devices: -1
  auto_select_gpus: true
  limit_train_batches: 100
  max_epochs: 200
  gradient_clip_val: 0
  auto_scale_batch_size: false
  auto_lr_find: false

arb:
   debug: false
   base_res: [512, 512]
   max_size: [768, 512]
   divisible: 64
   max_ar_error: 4
   min_dim: 256
   dim_limit: 1024

dataset:
  img_path: 
    - "https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/mmk.tgz"
  center_crop: false
  ucg: 0.0
  debug_arb: false
  reload_interval: 10

optimizer:
  name: torch.optim.AdamW
  params:
    lr: 5e-6
    weight_decay: 1e-2
    eps: 1e-8

lr_scheduler:
  name: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
  params:
    T_0: 10
    T_mult: 1
    eta_min: 7e-8
    last_epoch: -1

monitor:
  wandb_id: ""
  store_checkpoints: true